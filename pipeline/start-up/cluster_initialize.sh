#!/bin/bash

# Copy this script to GS bucket with:
# PIPELINEBUCKET="gs://blastgcp-pipeline-test"
# gsutil cp  cluster_initialize.sh "$PIPELINEBUCKET/scripts/cluster_initialize.sh"

cd /tmp

# Master or worker?
ROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)

if [[ "${ROLE}" == 'Master' ]]; then
    echo "master node"
    # Need maven to build jars, virtualenv for installing Google APIs for tests
    apt-get update -y
    #apt-get install -y -u maven python python-dev python3 python3-dev virtualenv chromium xterm protobuf-compiler telnet
    apt-get install -y -u maven python python-dev python3 python3-dev virtualenv telnet
else
    echo "worker node"
# Grant wider permissions to Blast Databases
    BLASTTMP=/mnt/1/blast/
    BLASTDBDIR=$BLASTTMP/db/
    mkdir -p $BLASTDBDIR

    cd $BLASTTMP
    chown -R spark:spark $BLASTTMP
    chmod -R ugo+rxw $BLASTTMP

    ls -laR $BLASTTMP
fi

# Install Stackdriver monitor and structured logging agents
#    curl -sSO https://dl.google.com/cloudagents/install-monitoring-agent.sh
#    sudo bash install-monitoring-agent.sh | tee -a stackdriver-install.log 2>&1

#    curl -sSO https://dl.google.com/cloudagents/install-logging-agent.sh
#    sudo bash install-logging-agent.sh --structured | tee -a stackdriver-install.log 2>&1

# Have fluent (also Stackdriver) pick up our log4j messages
    cd /tmp
    cat << DONE > libblast-log.conf
    <source>
    # Automatically generated by cluster_initialize.sh
    # TODO: regexp parser text to JSON payload /^(?<action>)\w+) (?<thing>\w+) ...
    # app:pipeline rid:12345 host: pid: thread: trace: span: ...
        @type tail
        format syslog
        # format /\[(?<severity>(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)\] (?<class>\w+) (?<pidthread>\w+) \$(?<msg>.+)/
        path /tmp/blastjni.*.log
        pos_file /var/tmp/fluentd.blastjni.pos
        read_from_head true
        tag blast-dev
    </source>
DONE
cp libblast-log.conf /etc/google-fluentd/config.d/libblast-log.conf
service google-fluentd restart

# Configure all master/worker nodes' log4j
    cat << 'DONE2' > log4j.proto
# Below automatically generated by cluster_initialize.sh

log4j.appender.tmpfile=org.apache.log4j.FileAppender
log4j.appender.tmpfile.File=/tmp/blastjni.${user.name}.log
log4j.appender.tmpfile.layout=org.apache.log4j.PatternLayout
log4j.appender.tmpfile.layout.ConversionPattern=%m%n

log4j.appender.sparkfile=org.apache.log4j.FileAppender
log4j.appender.sparkfile.File=/var/log/spark/blastjni.${user.name}.log
log4j.appender.sparkfile.layout=org.apache.log4j.PatternLayout
log4j.appender.sparkfile.layout.ConversionPattern=%d [%p] [%t] %c: app:pipeline %m%n

# Spark/JNI layers will further restrict on a per query basis
log4j.logger.gov.nih.nlm.ncbi.blastjni=ERROR, tmpfile, sparkfile
log4j.logger.gov.nih.nlm.ncbi.blastjni.BLAST_BENCH=ERROR, tmpfile
log4j.logger.gov.nih.nlm.ncbi.blastjni.BLAST_TEST=ERROR, tmpfile

DONE2
#    cat log4j.proto >> /etc/spark/conf.dist/log4j.properties

logger -t cluster_initialize.sh "BLASTJNI cluster_initialize.sh complete"
date

exit 0


# Future enhancements:
# run-init-actions-early? To get RAM before Spark/YARN?
# Cheap Chaos Monkey (shutdown -h +$RANDOM)
# Start daemons
# pre-warm databases
# Schedule things (cron or systemd timer)
# Configure user environments
# Submit stream, keep it alive:
#     https://github.com/GoogleCloudPlatform/dataproc-initialization-actions/tree/master/post-init

